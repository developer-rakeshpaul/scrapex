---
title: LLM Integration
description: Enhance scraped content with AI-powered summarization, entity extraction, and structured data.
---

import { Tabs, TabItem, Aside } from '@astrojs/starlight/components';

scrapex integrates with LLM providers to enhance scraped content with AI capabilities.

## Setting Up a Provider

<Tabs>
  <TabItem label="OpenAI">
    ```typescript
    import { scrape } from 'scrapex';
    import { createOpenAI } from 'scrapex/llm';

    const openai = createOpenAI({
      apiKey: process.env.OPENAI_API_KEY,
      model: 'gpt-4o-mini', // optional, defaults to gpt-4o-mini
    });
    ```
  </TabItem>
  <TabItem label="Anthropic">
    ```typescript
    import { scrape } from 'scrapex';
    import { createAnthropic } from 'scrapex/llm';

    const anthropic = createAnthropic({
      apiKey: process.env.ANTHROPIC_API_KEY,
      model: 'claude-3-5-haiku-20241022', // optional
    });
    ```
  </TabItem>
  <TabItem label="Ollama">
    ```typescript
    import { createOllama } from 'scrapex/llm';

    const ollama = createOllama({
      model: 'llama3.2',
    });
    ```
  </TabItem>
  <TabItem label="LM Studio">
    ```typescript
    import { createLMStudio } from 'scrapex/llm';

    const lmstudio = createLMStudio({
      model: 'local-model',
    });
    ```
  </TabItem>
</Tabs>

## Local & Self-Hosted Models

scrapex fully supports local LLM servers running in Docker or natively. Any server that exposes an OpenAI-compatible chat completions API works seamlessly.

### Docker Model Runner (Recommended)

[Docker Model Runner](https://docs.docker.com/ai/model-runner/) is the easiest way to run LLMs locally. It's built into Docker Desktop 4.40+ and provides an OpenAI-compatible API out of the box.

```bash
# Pull a model
docker model pull ai/llama3.2

# Run the model
docker model run ai/llama3.2
```

```typescript
import { createHttpLLM } from 'scrapex/llm';

// From your host machine (requires TCP host access enabled)
const provider = createHttpLLM({
  baseUrl: 'http://localhost:12434/engines/llama.cpp/v1/chat/completions',
  model: 'ai/llama3.2',
  requireHttps: false,
  allowPrivate: true,
});

const result = await scrape(url, {
  llm: provider,
  enhance: ['summarize'],
});
```

<Aside type="tip">
  Docker Model Runner is enabled by default in Docker Desktop 4.40+ on macOS Apple Silicon. Enable TCP host access in Docker Desktop settings to use `localhost:12434`.
</Aside>

#### Using from Docker Compose

With Docker Compose v2.35+, you can define models directly in your `compose.yml`:

```yaml
services:
  myapp:
    build: .
    depends_on:
      - llm
    environment:
      - LLM_BASE_URL=http://model-runner.docker.internal/engines/llama.cpp/v1/chat/completions

  llm:
    provider:
      type: model
      options:
        model: ai/llama3.2
```

```typescript
// From within a container, use the internal DNS name
const provider = createHttpLLM({
  baseUrl: process.env.LLM_BASE_URL,
  model: 'ai/llama3.2',
  requireHttps: false,
  allowPrivate: true,
});
```

### Running Ollama in Docker

[Ollama](https://ollama.ai) is another popular option for running local models:

```bash
# Start Ollama container
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama

# Pull a model
docker exec -it ollama ollama pull llama3.2
```

```typescript
import { createOllama } from 'scrapex/llm';

const provider = createOllama({
  model: 'llama3.2',
  baseUrl: 'http://localhost:11434/v1/chat/completions', // default
});

const result = await scrape(url, {
  llm: provider,
  enhance: ['summarize'],
});
```

### Other OpenAI-Compatible Servers

For other local servers like vLLM, LocalAI, or llama.cpp, use `createHttpLLM`:

<Tabs>
  <TabItem label="vLLM">
    ```bash
    docker run --runtime nvidia --gpus all \
      -v ~/.cache/huggingface:/root/.cache/huggingface \
      -p 8000:8000 \
      vllm/vllm-openai:latest \
      --model meta-llama/Llama-3.2-3B-Instruct
    ```

    ```typescript
    import { createHttpLLM } from 'scrapex/llm';

    const provider = createHttpLLM({
      baseUrl: 'http://localhost:8000/v1/chat/completions',
      model: 'meta-llama/Llama-3.2-3B-Instruct',
      requireHttps: false,
      allowPrivate: true,
    });
    ```
  </TabItem>
  <TabItem label="LocalAI">
    ```bash
    docker run -p 8080:8080 --name local-ai \
      -v models:/models \
      localai/localai:latest-cpu
    ```

    ```typescript
    import { createHttpLLM } from 'scrapex/llm';

    const provider = createHttpLLM({
      baseUrl: 'http://localhost:8080/v1/chat/completions',
      model: 'gpt-3.5-turbo',
      requireHttps: false,
      allowPrivate: true,
    });
    ```
  </TabItem>
  <TabItem label="llama.cpp">
    ```bash
    # Build and run llama.cpp server
    ./llama-server -m models/llama-3.2-3b.gguf --port 8080
    ```

    ```typescript
    import { createHttpLLM } from 'scrapex/llm';

    const provider = createHttpLLM({
      baseUrl: 'http://localhost:8080/v1/chat/completions',
      model: 'llama-3.2-3b',
      requireHttps: false,
      allowPrivate: true,
    });
    ```
  </TabItem>
</Tabs>

### Security Configuration

When connecting to local servers, you need to configure security options:

| Option | Default | Local Usage |
|--------|---------|-------------|
| `requireHttps` | `true` | Set to `false` for HTTP |
| `allowPrivate` | `false` | Set to `true` for localhost/Docker |

<Aside type="caution">
  Only disable HTTPS and allow private IPs when connecting to servers you control. These settings bypass SSRF protection.
</Aside>

The built-in presets `createOllama()` and `createLMStudio()` automatically configure these options for local development.

## Enhancing Scraped Content

Pass the provider and enhancement types to the `scrape()` function:

```typescript
const result = await scrape('https://example.com/article', {
  llm: openai,
  enhance: ['summarize', 'entities', 'tags'],
});

console.log(result.summary);        // AI-generated summary
console.log(result.entities);       // Extracted entities
console.log(result.suggestedTags);  // Suggested tags
```

## Enhancement Types

### Summarize

Generate a concise summary of the content:

```typescript
const result = await scrape(url, { llm: provider, enhance: ['summarize'] });
console.log(result.summary);
// "This article discusses the latest developments in..."
```

### Extract Entities

Extract named entities (people, organizations, locations, etc.):

```typescript
const result = await scrape(url, { llm: provider, enhance: ['entities'] });
console.log(result.entities);
// {
//   people: ['John Smith', 'Jane Doe'],
//   organizations: ['Acme Corp', 'TechCo'],
//   locations: ['San Francisco', 'New York'],
//   technologies: ['React', 'Node.js'],
// }
```

### Classify

Classify content type (article, repo, docs, etc.):

```typescript
const result = await scrape(url, { llm: provider, enhance: ['classify'] });
console.log(result.contentType);
// 'article', 'repo', 'docs', etc.
```

### Extract Structured Data

Extract specific data using a JSON schema:

```typescript
const result = await scrape(url, {
  llm: provider,
  extract: {
    productName: 'string',
    price: 'number',
    features: 'string[]',
  },
});
console.log(result.extracted);
// { productName: 'Widget Pro', price: 99.99, features: ['Fast', 'Reliable'] }
```

## Chaining Enhancements

Chain multiple enhancements together:

```typescript
const result = await scrape(url, {
  llm: provider,
  enhance: ['summarize', 'entities', 'tags', 'classify'],
  extract: {
    author: 'string',
    topics: 'string[]',
  },
});
```

<Aside type="note">
  Enhancements run in parallel where possible, but each type can add additional API calls and tokens.
</Aside>

## Token Usage

Each enhancement makes one or more LLM API calls. To minimize costs:

- Use `textContent` instead of `content` (plain text uses fewer tokens)
- Set `maxContentLength` to limit input size
- Use smaller/faster models for simple tasks

```typescript
const result = await scrape(url, {
  maxContentLength: 10000, // Limit content to 10k chars
  llm: provider,
  enhance: ['summarize'],
});
```

## Custom Questions with ask()

Use `ask()` for ad-hoc questions about scraped content:

```typescript
import { scrape } from 'scrapex';
import { createOpenAI, ask } from 'scrapex/llm';

const provider = createOpenAI({ model: 'gpt-4o-mini' });
const data = await scrape('https://example.com/article');

// Simple question - returns string
const result = await ask(data, provider, "What is the main argument?", {
  key: 'mainArgument'
});
console.log(result.custom?.mainArgument);
// "The author argues that..."

// Structured response - returns typed object
const sentiment = await ask(data, provider, "Analyze the sentiment", {
  key: 'sentiment',
  schema: { tone: 'string', score: 'number', reasoning: 'string' }
});
console.log(sentiment.custom?.sentiment);
// { tone: 'positive', score: 0.8, reasoning: '...' }
```

### Template Placeholders

Use placeholders in prompts for dynamic content:

```typescript
const result = await ask(data, provider,
  "For {{domain}}: What are the key takeaways from '{{title}}'?",
  { key: 'takeaways' }
);
```

Available placeholders:
- `{{title}}` - Page title
- `{{url}}` - Full URL
- `{{content}}` - Main content
- `{{description}}` - Meta description
- `{{excerpt}}` - Content excerpt
- `{{domain}}` - Hostname only

## Error Handling

LLM enhancements can fail without failing the entire scrape:

```typescript
const result = await scrape(url, { llm: provider, enhance: ['summarize'] });

if (result.summary) {
  console.log('Summary:', result.summary);
} else {
  console.log('Summary enhancement failed or was skipped');
}
```

## Next Steps

- [Custom Extractors](/guides/custom-extractors) - Create domain-specific extractors
- [API Reference: LLM Providers](/api/llm-providers) - Full provider API documentation
